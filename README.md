# micrograd

nn training under the hood

implements backpropagation

whats backprop? its an algorithm that evaluates gradient of a loss function w.r.t the weights of a nn and that allows us to tune the weights of the nn in such a way that the loss is minimized and in turn improve the accuracy of the nn

